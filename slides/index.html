<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>We need to talk about non-linearity</title>
    <meta charset="utf-8" />
    <meta name="author" content="Gavin Simpson" />
    <meta name="date" content="2023-10-03" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: inverse, middle, left, my-title-slide, title-slide

.title[
# We need to talk about non-linearity
]
.author[
### Gavin Simpson
]
.institute[
### Department of Animal &amp; Veterinary Sciences, Aarhus University
]
.date[
### October 3, 2023
]

---

class: inverse middle center subsection



# Slides

&lt;img src="resources/bit.ly_princeton-gams.png" width="50%" style="display: block; margin: auto;" /&gt;

---
class: inverse middle center subsection

# Nonlinearity

---

# HadCRUT4 time series

![](index_files/figure-html/hadcrut-temp-example-1.svg)&lt;!-- --&gt;

???

Hadley Centre NH temperature record ensemble

How would you model the trend in these data?

---

# Linear Models

`$$y_i \sim \mathcal{N}(\mu_i, \sigma^2)$$`

`$$\mu_i = \beta_0 + \beta_1 \mathtt{year}_{i} + \beta_2 \mathtt{year}^2_{1i} + \cdots + \beta_j \mathtt{year}^j_{i}$$`

---

# Polynomials perhaps&amp;hellip;

![](index_files/figure-html/hadcrut-temp-polynomial-1.svg)&lt;!-- --&gt;

---

# Polynomials perhaps&amp;hellip;

We can keep on adding ever more powers of `\(\boldsymbol{x}\)` to the model &amp;mdash;
model selection problem

**Runge phenomenon** &amp;mdash; oscillations at the edges of an interval &amp;mdash;
means simply moving to higher-order polynomials doesn't always improve accuracy

---
class: inverse middle center subsection

# GAMs offer a solution

---

# Generalized Additive Models

&lt;br /&gt;

![](resources/tradeoff-slider.png)

.references[Source: [GAMs in R by Noam Ross](https://noamross.github.io/gams-in-r-course/)]

???

GAMs are an intermediate-complexity model

* can learn from data without needing to be informed by the user
* remain interpretable because we can visualize the fitted features

---

# How is a GAM different?

`$$\begin{align*}
y_i &amp;\sim \mathcal{D}(\mu_i, \theta) \\ 
\mathbb{E}(y_i) &amp;= \mu_i = g(\eta_i)^{-1}
\end{align*}$$`

We model the mean of data as a sum of linear terms:

`$$\eta_i = \beta_0 +\sum_j \color{red}{ \beta_j x_{ji}} +\epsilon_i$$`

A GAM is a sum of _smooth functions_ or _smooths_

`$$\eta_i = \beta_0 + \sum_j \color{red}{f_j(x_{ji})} + \epsilon_i$$`

---

# Fitting a GAM in R

```r
model &lt;- gam(y ~ s(x1) + s(x2) + te(x3, x4), # formuala describing model
             data = my_data_frame,           # your data
             method = "REML",                # or "ML"
             family = gaussian)              # or something more exotic
```

`s()` terms are smooths of one or more variables

`te()` terms are the smooth equivalent of *main effects + interactions*

`$$\eta_i = \beta_0 + f(\mathtt{Year}_i)$$`

```r
library(mgcv)
gam(Temperature ~ s(Year, k = 10), data = gtemp, method = 'REML')
```

---

# Fitted GAM

![](index_files/figure-html/hadcrtemp-plot-gam-1.svg)&lt;!-- --&gt;

---

# How did `gam()` *know* what line to fit?

![](index_files/figure-html/hadcrtemp-plot-gam-1.svg)&lt;!-- --&gt;

---



# Wiggly things

.center[![](resources/spline-anim.gif)]

???

GAMs use splines to represent the non-linear relationships between covariates,
here `x`, and the response variable on the `y` axis.

---

# Basis expansions

In the polynomial models we used a polynomial basis expansion of `\(\boldsymbol{x}\)`

* `\(\boldsymbol{x}^0 = \boldsymbol{1}\)` &amp;mdash; the model constant term
* `\(\boldsymbol{x}^1 = \boldsymbol{x}\)` &amp;mdash; linear term
* `\(\boldsymbol{x}^2\)`
* `\(\boldsymbol{x}^3\)`
* &amp;hellip;

---

# Splines

Splines are *functions* composed of simpler functions

Simpler functions are *basis functions* &amp; the set of basis functions is a *basis*

When we model using splines, each basis function `\(b_k\)` has a coefficient `\(\beta_k\)`

Resultant spline is a the sum of these weighted basis functions, evaluated at the values of `\(x\)`

`$$f(x) = \sum_{k = 1}^K \beta_k b_k(x)$$`

---

# Splines formed from basis functions

![](index_files/figure-html/basis-functions-1.svg)&lt;!-- --&gt;

???

Splines are built up from basis functions

Here I'm showing a cubic regression spline basis with 10 knots/functions

We weight each basis function to get a spline. Here all the basisi functions have the same weight so they would fit a horizontal line

---

# Weight basis functions &amp;#8680; spline



.center[![](resources/basis-fun-anim.gif)]

???

But if we choose different weights we get more wiggly spline

Each of the splines I showed you earlier are all generated from the same basis functions but using different weights

---

# How do GAMs learn from data?

![](index_files/figure-html/example-data-figure-1.svg)&lt;!-- --&gt;

???

How does this help us learn from data?

Here I'm showing a simulated data set, where the data are drawn from the orange functions, with noise. We want to learn the orange function from the data

---

# Maximise penalised log-likelihood &amp;#8680; &amp;beta;



.center[![](resources/gam-crs-animation.gif)]

???

Fitting a GAM involves finding the weights for the basis functions that produce a spline that fits the data best, subject to some constraints


---
class: inverse middle center subsection

# Avoid overfitting our sample

---
class: inverse center middle large-subsection

# How wiggly?

$$
\int_{\mathbb{R}} [f^{\prime\prime}]^2 dx = \boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}
$$

---
class: inverse center middle large-subsection

# Penalised fit

$$
\mathcal{L}_p(\boldsymbol{\beta}) = \mathcal{L}(\boldsymbol{\beta}) - \frac{1}{2} \lambda\boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}
$$

---

# Wiggliness

`$$\int_{\mathbb{R}} [f^{\prime\prime}]^2 dx = \boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta} = \large{W}$$`

(Wiggliness is the correct word and I'll die on that hill Reviewer 2)

We penalize wiggliness to avoid overfitting

---

# Making wiggliness matter

`\(W\)` measures **wiggliness**

(log) likelihood measures closeness to the data

We use a **smoothing parameter** `\(\lambda\)` to define the trade-off, to find
the spline coefficients `\(B_k\)` that maximize the **penalized** log-likelihood

`$$\mathcal{L}_p = \log(\text{Likelihood})  - \lambda W$$`

---

# HadCRUT4 time series

![](index_files/figure-html/hadcrut-temp-penalty-1.svg)&lt;!-- --&gt;

---

# Picking the right wiggliness

.pull-left[
2 ways to think about how to optimize `\(\lambda\)`:

* Predictive: Minimize out-of-sample error
* Bayesian:  Put priors on our basis coefficients
]

.pull-right[
* **Practically**: use **REML**, because of numerical stability
* (Figure modified from Wood, 2011, J. R. Stat. Soc. B)
]

.center[
![REML and GCV scores](./resources/remlgcv.png)
]

---

# Maximum allowed wiggliness

We set **basis complexity** or "size" `\(k\)`

This is _maximum wigglyness_, can be thought of as number of small functions that make up a curve

Once smoothing is applied, curves have fewer **effective degrees of freedom (EDF)**

EDF &lt; `\(k\)`

---

# Maximum allowed wiggliness

`\(k\)` must be *large enough*, the `\(\lambda\)` penalty does the rest

*Large enough* &amp;mdash; space of functions representable by the basis includes the true function or a close approximation to the tru function

Bigger `\(k\)` increases computational cost

In **mgcv**, default `\(k\)` values are arbitrary &amp;mdash; after choosing the model terms, this is the key user choice

**Must be checked!** &amp;mdash; `mgcv::k.check()`


---

# GAM summary

1. GAMs give us a framework to model flexible nonlinear relationships

2. Use little functions (**basis functions**) to make big functions (**smooths**)

3. Use a **penalty** to trade off wiggliness/generality 

4. Need to make sure your smooths are **wiggly enough**

---
class: inverse subsection center middle

# Examples

---

# Vocal charades

.row[
.col-6[

Example from Winter &amp; Wieling (2016) https://doi.org/10.1093/jole/lzv003

Participants had to vocalize a meaning to their partner without using language

Partner had to guess the meaning

Over time, participants converge on more iconic vocalizations

After more time, vocalizations become more idiosyncratics

]

.col-6[

![](index_files/figure-html/plot-dyads-1.svg)&lt;!-- --&gt;

]
]

???

Cut and paste from Winter &amp; Wieling (2016) https://doi.org/10.1093/jole/lzv003

Consider an experiment where participants play a game of ‘vocal charades’, as in the study of Perlman et al. (2015). At each round, a participant has to vocalize a meaning to the partner (e.g. ‘ugly’) without using language (e.g. through grunting or hissing). The partner has to guess the meaning of the vocalization. This game is played repeatedly with the finding that over time, a dyad converges on a set of nonlinguistic vocalizations that assure a high degree of intelligibility between the two participants in the dyad (Perlman et al. 2015). Initially, participants may be struggling with the task and explore very different kinds of vocalizations. Over time, they may converge on a more stable set of iconic vocalizations, that is vocalizations that resemble the intended referent (e.g. a high-pitched sound for ‘attractive’ and a low-pitched sound for ‘ugly’).

Finally, after even more time, the dyad may conventionalize to idiosyncratic patterns that deviate from iconicity and become increasingly arbitrary (cf. Garrod et al. 2007). This general pattern is shown with simulated data in Figure 3a, with iconicity first increasing, and then decreasing slightly, as signals become more and more arbitrary through conventionalization.5 Such an inverse U-shaped pattern can be modeled by incorporating polynomial fixed effects into the mixed-effects regression analysis. This approach is frequently called GCA in psychology (Mirman et al. 2008; Mirman 2014).

---

# Parametric model

$$
\mathtt{iconicity}_i = \alpha + \beta_1 t_i + \beta_2 t^2_i + \dots
$$

(Ignoring the dyads; random effects for `\(t\)` &amp; `\(t^2\)` and `\(\alpha\)`)

---

# GAM approach

`$$\mathtt{iconicity}_{i} = \alpha_{i(\mathtt{dyad})} + f_{i(\mathtt{dyad})}(\mathtt{t})$$`

Where now I have included random effects and a "random smooth" for the `\(j\)`th dyad

---

# GAM approach


```r
m_0 &lt;- bam(iconicity ~ s(t, dyad, bs = "fs", k = 7),
  data = dyads, method = "fREML")
```

---

# GAM approach

![](index_files/figure-html/plot-dyad-model-1-1.svg)&lt;!-- --&gt;

---

# HGAM approach

Using a lot of degrees of freedom (parameters) to model essentially the same _shape_

Instead, we can fit what Pedersen et al (2019) termed a Hierarchical GAM


```r
m_1 &lt;- bam(iconicity ~ s(t, k = 6) +
  s(t, dyad, bs = "fs", k = 6),
data = dyads, method = "fREML")
```

---

# HGAM approach

![](index_files/figure-html/plot-dyad-model-2-1.svg)&lt;!-- --&gt;

---

# HGAM approach

The HGAM formulation uses many fewer degrees of freedom for effectively the same fit


```
## # A tibble: 2 × 2
##   model   edf
##   &lt;chr&gt; &lt;dbl&gt;
## 1 m_0    47.0
## 2 m_1    30.6
```


---
class: inverse subsection center middle

# Lyme borreliosis



---

# Lyme borreliosis

.row[

.col-9[
Lyme disease is a zoonotic infection

Caused by bacteria (spirochetes) in the *Borrelia burgdorferi* sensu lato complex

Transmitted by infected tick larvae &amp; nymphs in the genus *Ixodes* during blood meals

Common vector-borne disease in N hemisphere

Reported cases of Lyme borreliosis have increased in recent decades
]

.col-3[

.center[
&lt;img src="resources/Adult_deer_tick.jpg" width="1763" /&gt;
]

.smaller[Adult deer tick, *Ixodes scapularis*

(Source: Scott Bauer)
]

]

]
???

Increased in both number and geographical range

---

# Tick-borne disease &amp; climate

.center[
&lt;img src="resources/gilbert-tick-zoonoses-schematic.png" width="90%" /&gt;
]

.smaller[source: Gilbert (2021) *Annu. Rev. Entomol.* [10.1146/annurev-ento-052720-094533](https://doi.org/10.1146/annurev-ento-052720-094533)
]

???

Figure caption from Gilbert (2021)

Schematic diagram showing how climate change can affect ticks directly (by changing oviposition, development, mortality rates, and activity) and indirectly (by changing habitat and host species and abundance). Climate change can, in turn, affect tick-borne pathogen infection risk in humans by directly affecting human behavior (e.g., outdoor recreation) and indirectly affecting pathogen transmission rates and prevalence via hosts and ticks. The relative importance of each pathway is challenging to ascertain.


---

# Tick-borne disease &amp; climate

Aim of Goren *et al* (2023) was to study

1. the incidence, and

2. the seasonal timing

of Lyme borreliosis at the northern limit of the range in Europe

---

# Cases of Lyme borreliosis in Norway

&lt;img src="index_files/figure-html/lyme-cases-plot-1.svg" width="95%" style="display: block; margin: auto;" /&gt;
.small[
Data: Goren *et al* (2023) *Proc. Biol. Sci.* [10.1098/rspb.2022.2420](https://doi.org/10.1098/rspb.2022.2420)
]

---

# Climate change in Norway

Compared to 1979&amp;ndash;2008 reference period

Annual mean temperature increased by ~ 0.5&amp;#8451;

Winter temperatures increase by ~1&amp;#8451;

Growing season increased by 1&amp;ndash;2 weeks nationally

Annual precipitation increased by ~3% per decade

---

# GAM for seasonal data

Assume weekly cases are distributed Negative binomial

$$
\texttt{cases}_i \sim \mathcal{NB}(\mu_i, \theta)
$$

Lots of ways to decompose these data; settled on

.small[
$$
\log (\mathbb{E}(\texttt{cases}_i) ) = \beta_0 + f(\texttt{week}_i , \texttt{year}_i) + \log(\texttt{population}_i)
$$
]

where

$$
f(\texttt{week}_i , \texttt{year}_i)
$$

is a tensor product smooth (main effects plus interaction)

---

# GAM for seasonal data

Model code looks like:


```r
m &lt;- bam(cases ~ s(week, bs = "cc", k = 20) +
                 s(year, k = 25) +
                 ti(week, year, bs = c("cc", "tp"), k = c(20, 20)) +
                 offset(log(pop)),
    data = lyme,
    family = nb(),
    method = "fREML",
    knots = list(week = c(0.5, 52.5)),
    discrete = TRUE,
    nthreads = 4)
```

Using lots of *mgcv* tricks to fit the model *fast* as some decompositions take a long time to fit with `gam()`

Decomposed tensor product into main smooth effects plus a pure smooth interaction

---

# Estimated smooth functions

&lt;img src="index_files/figure-html/lyme-draw-1.svg" width="95%" style="display: block; margin: auto;" /&gt;

---

# Estimated seasonal curves

&lt;img src="index_files/figure-html/lyme-plot-fitted-curves-1.svg" width="95%" style="display: block; margin: auto;" /&gt;

---

# How has timing of peak changed?

Easy &amp;mdash; using data in previous figure, find week in each year where `\(\widehat{\texttt{cases}}_{ij}\)` is maximal

--

Harder &amp;mdash; account for the uncertainty in the model &amp; put an uncertainty band on `\(\widehat{\texttt{peak}}_{ij}\)`

--

GAMs are an *empirical Bayesian* model so we can simulate from the posterior distribution of the model to get answers to questions like this

---

# How has timing of peak changed?

*gratia* 📦 has functions to make doing this easy(-ish)


```r
ds2 &lt;- data_slice(m, week = unique(week), year = unique(year), pop = log(100000))

ds2 &lt;- ds2 |&gt;
    add_column(row = seq_len(nrow(ds2)), .before = 1L)

fs2 &lt;- fitted_samples(m, data = ds2, n = 10000, seed = 42, scale = "response")

fs2 &lt;- fs2 |&gt;
    left_join(ds2, by = join_by(row)) |&gt;
    select(-pop)

peak_week &lt;- fs2 |&gt;
    group_by(year, draw) |&gt;
    slice_max(order_by = fitted, n = 1) |&gt;
    group_by(year) |&gt;
    summarise(peak = quantile(week, prob = 0.5),
        lower_ci = quantile(week, prob = 0.055),
        upper_ci = quantile(week, prob = 0.945))
```

---

# How has timing of peak changed?

&lt;img src="index_files/figure-html/lyme-plot-peak-week-1.svg" width="95%" style="display: block; margin: auto;" /&gt;

---

# Summary

GAMs offer a flexible way to model (decompose) temporal variation in ecological time series

Can be adapted in many ways

* space
* space &amp; time
* covariate effects
* model distributions or quantiles
* handle very large data

Posterior inference allows us to do many useful things (easily &amp;mdash; more easily)

---

# Slides

* &amp;copy; Simpson (2022-2023) [![Creative Commons Licence](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)

* HTML Slide deck [bit.ly/princeton-gams](https://bit.ly/princeton-gams)

* RMarkdown [https://github.com/gavinsimpson/princeton-2023](https://github.com/gavinsimpson/princeton-2023)

* @ucfagls on Twitter
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
